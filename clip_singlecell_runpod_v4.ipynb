{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXJyVNS2wG1B"
   },
   "source": [
    "# Downloads and Instalation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emgZDu0XwEgO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vNf4bCRzqNx_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyfaidx\n",
      "  Downloading pyfaidx-0.8.1.4-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pyfaidx) (24.1)\n",
      "Downloading pyfaidx-0.8.1.4-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: pyfaidx\n",
      "Successfully installed pyfaidx-0.8.1.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m--2025-06-04 15:14:04--  https://hgdownload.soe.ucsc.edu/goldenpath/hg38/bigZips/hg38.fa.gz\n",
      "Resolving hgdownload.soe.ucsc.edu (hgdownload.soe.ucsc.edu)... 128.114.119.163\n",
      "Connecting to hgdownload.soe.ucsc.edu (hgdownload.soe.ucsc.edu)|128.114.119.163|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 983659424 (938M) [application/x-gzip]\n",
      "Saving to: ‘hg38.fa.gz’\n",
      "\n",
      "hg38.fa.gz          100%[===================>] 938.09M  17.8MB/s    in 59s     \n",
      "\n",
      "2025-06-04 15:15:03 (16.0 MB/s) - ‘hg38.fa.gz’ saved [983659424/983659424]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install pyfaidx\n",
    "!wget https://hgdownload.soe.ucsc.edu/goldenpath/hg38/bigZips/hg38.fa.gz\n",
    "!gunzip -d hg38.fa.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PxoUXOjwdTNq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-06-04 15:15:28--  https://ftp.ncbi.nlm.nih.gov/geo/samples/GSM8015nnn/GSM8015425/suppl/GSM8015425%5F3T3%5FK562%5FATAC%5Fpeak%5Fcount%5Fmatrix%5Fhuman.mtx.gz\n",
      "Resolving ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)... 130.14.250.7, 130.14.250.13, 130.14.250.31, ...\n",
      "Connecting to ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)|130.14.250.7|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 111703259 (107M) [application/x-gzip]\n",
      "Saving to: ‘GSM8015425_3T3_K562_ATAC_peak_count_matrix_human.mtx.gz’\n",
      "\n",
      "GSM8015425_3T3_K562 100%[===================>] 106.53M  28.1MB/s    in 4.0s    \n",
      "\n",
      "2025-06-04 15:15:32 (26.8 MB/s) - ‘GSM8015425_3T3_K562_ATAC_peak_count_matrix_human.mtx.gz’ saved [111703259/111703259]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://ftp.ncbi.nlm.nih.gov/geo/samples/GSM8015nnn/GSM8015425/suppl/GSM8015425%5F3T3%5FK562%5FATAC%5Fpeak%5Fcount%5Fmatrix%5Fhuman.mtx.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZwwgQ1Igd0RU"
   },
   "outputs": [],
   "source": [
    "!gunzip -d GSM8015425_3T3_K562_ATAC_peak_count_matrix_human.mtx.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LKaFUutseH4C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-06-04 15:15:36--  https://ftp.ncbi.nlm.nih.gov/geo/samples/GSM8015nnn/GSM8015425/suppl/GSM8015425%5F3T3%5FK562%5FATAC%5Fbarcodes%5Fhuman.tsv.gz\n",
      "Resolving ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)... 130.14.250.10, 130.14.250.11, 130.14.250.12, ...\n",
      "Connecting to ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)|130.14.250.10|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 26548 (26K) [application/x-gzip]\n",
      "Saving to: ‘GSM8015425_3T3_K562_ATAC_barcodes_human.tsv.gz’\n",
      "\n",
      "GSM8015425_3T3_K562 100%[===================>]  25.93K  --.-KB/s    in 0.09s   \n",
      "\n",
      "2025-06-04 15:15:38 (283 KB/s) - ‘GSM8015425_3T3_K562_ATAC_barcodes_human.tsv.gz’ saved [26548/26548]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://ftp.ncbi.nlm.nih.gov/geo/samples/GSM8015nnn/GSM8015425/suppl/GSM8015425%5F3T3%5FK562%5FATAC%5Fbarcodes%5Fhuman.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zVgCMhnUeJbg"
   },
   "outputs": [],
   "source": [
    "!gunzip -d GSM8015425_3T3_K562_ATAC_barcodes_human.tsv.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kDjUE6wQeZ7n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-06-04 15:15:38--  https://ftp.ncbi.nlm.nih.gov/geo/samples/GSM8015nnn/GSM8015425/suppl/GSM8015425%5F3T3%5FK562%5FATAC%5Ffeatures%5Fhuman.tsv.gz\n",
      "Resolving ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)... 130.14.250.7, 130.14.250.10, 130.14.250.11, ...\n",
      "Connecting to ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)|130.14.250.7|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1051513 (1.0M) [application/x-gzip]\n",
      "Saving to: ‘GSM8015425_3T3_K562_ATAC_features_human.tsv.gz’\n",
      "\n",
      "GSM8015425_3T3_K562 100%[===================>]   1.00M  1.82MB/s    in 0.6s    \n",
      "\n",
      "2025-06-04 15:15:41 (1.82 MB/s) - ‘GSM8015425_3T3_K562_ATAC_features_human.tsv.gz’ saved [1051513/1051513]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://ftp.ncbi.nlm.nih.gov/geo/samples/GSM8015nnn/GSM8015425/suppl/GSM8015425%5F3T3%5FK562%5FATAC%5Ffeatures%5Fhuman.tsv.gz\n",
    "!gunzip -d GSM8015425_3T3_K562_ATAC_features_human.tsv.gz\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4lQVlytJeMbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [pandas]2m2/3\u001b[0m [pandas]\n",
      "\u001b[1A\u001b[2KSuccessfully installed pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting scipy\n",
      "  Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (1.26.3)\n",
      "Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m116.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scipy\n",
      "Successfully installed scipy-1.15.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting matplotlib\n",
      "  Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.58.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (106 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.58.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m122.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [matplotlib]5\u001b[0m [matplotlib]\n",
      "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.1 kiwisolver-1.4.8 matplotlib-3.10.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install scipy\n",
    "!pip install matplotlib\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vit-pytorch\n",
      "  Downloading vit_pytorch-1.10.1-py3-none-any.whl.metadata (69 kB)\n",
      "Collecting einops>=0.7.0 (from vit-pytorch)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.11/dist-packages (from vit-pytorch) (2.4.1+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from vit-pytorch) (0.19.1+cu124)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.2.65 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (12.4.2.65)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.0.44 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (11.2.0.44)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.119 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (10.3.5.119)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.0.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (11.6.0.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.0.142 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (12.3.0.142)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (12.4.99)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (12.4.99)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10->vit-pytorch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.10->vit-pytorch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->vit-pytorch) (1.26.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->vit-pytorch) (10.2.0)\n",
      "Downloading vit_pytorch-1.10.1-py3-none-any.whl (140 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Installing collected packages: einops, vit-pytorch\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [vit-pytorch]\u001b[0m [vit-pytorch]\n",
      "\u001b[1A\u001b[2KSuccessfully installed einops-0.8.1 vit-pytorch-1.10.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install vit-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZ56DgrzwOA3"
   },
   "source": [
    "# Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3fNCh_5wNIx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2xZdm240dv77"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3N8Y9v5weNMG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "EXsUOwECeRb3",
    "outputId": "a0cce6af-dc3b-41a6-e2bb-287a28d7c0de"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chr1-821036-821536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chr1-825646-826146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chr1-826592-827092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chr1-827283-827783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chr1-832045-832545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131956</th>\n",
       "      <td>chrX-155886717-155887217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131957</th>\n",
       "      <td>chrX-155893471-155893971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131958</th>\n",
       "      <td>chrX-155944832-155945332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131959</th>\n",
       "      <td>chrX-155958658-155959158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131960</th>\n",
       "      <td>chrX-155968135-155968635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131961 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               0\n",
       "0             chr1-821036-821536\n",
       "1             chr1-825646-826146\n",
       "2             chr1-826592-827092\n",
       "3             chr1-827283-827783\n",
       "4             chr1-832045-832545\n",
       "...                          ...\n",
       "131956  chrX-155886717-155887217\n",
       "131957  chrX-155893471-155893971\n",
       "131958  chrX-155944832-155945332\n",
       "131959  chrX-155958658-155959158\n",
       "131960  chrX-155968135-155968635\n",
       "\n",
       "[131961 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5XPnY1ZBfIb8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "vwHlT-0zd4OR"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "BWY-KQyyd-BQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.io\n",
    "df_bar = pd.read_csv('GSM8015425_3T3_K562_ATAC_barcodes_human.tsv', sep='\\t', header=None)\n",
    "df_feature = pd.read_csv('GSM8015425_3T3_K562_ATAC_features_human.tsv', sep='\\t', header=None)\n",
    "df_feature\n",
    "mtx_file = 'GSM8015425_3T3_K562_ATAC_peak_count_matrix_human.mtx'\n",
    "sparse_matrix = scipy.io.mmread(mtx_file)\n",
    "df = pd.DataFrame.sparse.from_spmatrix(sparse_matrix)\n",
    "df.columns = df_bar[0]\n",
    "df.index = df_feature[0]\n",
    "df = df.T\n",
    "binary_df = df.gt(0).astype('Sparse[int]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "UebvBYGXib7s"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "y5YCp2mrpGQK"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from pyfaidx import Fasta\n",
    "\n",
    "\n",
    "GENOME = 'hg38.fa'\n",
    "genome = Fasta(GENOME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "e-24Nd6kIjoY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "GENOME = 'hg38.fa'\n",
    "genome = Fasta(GENOME)\n",
    "\n",
    "def extract_top_binary_patterns(binary_df, chrom: str, start: int, end: int,\n",
    "                                window_size: int = 50000, bin_size: int = 500,\n",
    "                                top_n: int = 10, plot: bool = False):\n",
    "    \"\"\"\n",
    "    Extract most frequent binary patterns in a region around a coordinate.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, int]]: Top N binary patterns and their frequencies, padded with zeros if needed.\n",
    "    \"\"\"\n",
    "    def parse_coords(col_name):\n",
    "        match = re.match(r\"(chr[^\\-]+)-(\\d+)-(\\d+)\", col_name)\n",
    "        if match:\n",
    "            c, s, e = match.groups()\n",
    "            center = (int(s) + int(e)) // 2\n",
    "            return c, center\n",
    "        return None, None\n",
    "\n",
    "    parsed = [parse_coords(col) for col in binary_df.columns]\n",
    "    coord_df = pd.DataFrame(parsed, columns=[\"chrom\", \"center\"])\n",
    "    coord_df[\"colname\"] = binary_df.columns\n",
    "\n",
    "    center = (start + end) // 2\n",
    "    half_window = window_size // 2\n",
    "    win_start, win_end = center - half_window, center + half_window\n",
    "    bins = np.arange(win_start, win_end, bin_size)\n",
    "    num_bins = len(bins)\n",
    "\n",
    "    in_window = coord_df[\n",
    "        (coord_df[\"chrom\"] == chrom) &\n",
    "        (coord_df[\"center\"] >= win_start) &\n",
    "        (coord_df[\"center\"] < win_end)\n",
    "    ].copy()\n",
    "\n",
    "    if in_window.empty:\n",
    "        return [('0' * num_bins, 0)] * top_n\n",
    "\n",
    "    in_window[\"bin\"] = ((in_window[\"center\"] - win_start) // bin_size).astype(int)\n",
    "\n",
    "    bin_matrix = np.zeros((binary_df.shape[0], num_bins), dtype=int)\n",
    "\n",
    "    for _, row in in_window.iterrows():\n",
    "        col_idx = binary_df.columns.get_loc(row[\"colname\"])\n",
    "        bin_matrix[:, row[\"bin\"]] += binary_df.iloc[:, col_idx].values\n",
    "\n",
    "    bin_matrix = (bin_matrix > 0).astype(int)\n",
    "\n",
    "    patterns = [\"\".join(map(str, row)) for row in bin_matrix]\n",
    "    pattern_counts = Counter(patterns)\n",
    "    top_patterns = pattern_counts.most_common(top_n)\n",
    "\n",
    "    # Pad with zero patterns if needed\n",
    "    if len(top_patterns) < top_n:\n",
    "        top_patterns += [('0' * num_bins, 0)] * (top_n - len(top_patterns))\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for i, (pattern, count) in enumerate(top_patterns):\n",
    "            plt.barh(i, count)\n",
    "            plt.text(count + 2, i, pattern, va='center', fontsize=8)\n",
    "        plt.yticks(range(top_n), [f'Pattern {i+1}' for i in range(top_n)])\n",
    "        plt.xlabel(\"Frequency\")\n",
    "        plt.title(f\"Top {top_n} Binary Patterns around {chrom}:{center:,} (±{half_window:,}bp)\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return top_patterns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "hot_encoder = dict(zip('ACTGN'[::], np.eye(5)))\n",
    "def convert_seq(seq):\n",
    "\n",
    "  return np.array([ hot_encoder[s] for s in seq.upper()]).T\n",
    "\n",
    "\n",
    "def get_sequence_from_fasta( chrom: str, start: int, end: int) -> str:\n",
    "    \"\"\"\n",
    "    Extracts a DNA sequence from a FASTA file given a genomic coordinate.\n",
    "\n",
    "    Args:\n",
    "        chrom (str): Chromosome name, e.g., \"chr1\".\n",
    "        start (int): 0-based start coordinate.\n",
    "        end (int): End coordinate (non-inclusive).\n",
    "\n",
    "    Returns:\n",
    "        str: DNA sequence as a string (uppercase).\n",
    "    \"\"\"\n",
    "    seq = genome[chrom][start:end].seq\n",
    "    seq = str(seq).upper()\n",
    "    return   seq, convert_seq(seq)\n",
    "\n",
    "\n",
    "\n",
    "def get_clip_encoding(chrom, start,end):\n",
    "  top_patterns = extract_top_binary_patterns(binary_df, chrom=chrom, start=start, end=end)\n",
    "  seq_and_hot = get_sequence_from_fasta(chrom, start, end)\n",
    "  top_out_process = np.array([[int(x) for x in x[0] ] for x in top_patterns])\n",
    "  vector_amounts = np.array([x[1] for x in top_patterns])\n",
    "  return seq_and_hot[0], seq_and_hot[1], top_out_process, vector_amounts\n",
    "\n",
    "\n",
    "seq, seq_h_out, top_out,top_amount = get_clip_encoding('chr1', 100_000_000, 100_000_032)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1KPVV9UfpYJd",
    "outputId": "a4ea1ab2-6e78-4faf-dced-78939661e737",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('TTCATATCTTAGCTATTGTGAACAATGCTGCA',\n",
       " array([[0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0.],\n",
       "        [1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
       "         1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " array([3525,  380,  274,   38,    0,    0,    0,    0,    0,    0]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq, seq_h_out, top_out,top_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "VnCQUbxGdUVr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ACTAAGCACACAGAGAATAATGTCTAGAATCT',\n",
       " array([[1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "         1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sequence_from_fasta('chr1', 100000, 100032)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "FGNhb_gydtxt"
   },
   "outputs": [],
   "source": [
    "seq, seq_h_out, top_out,top_amount = get_clip_encoding('chr1', 100_000_000, 100_000_032)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "IRWIqljiVZ7y"
   },
   "outputs": [],
   "source": [
    "#seq_h_out, top_out,top_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "mTN1b81hlKOI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_h_out, top_out,top_amount\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((5, 32), (10, 100), (10,))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('seq_h_out, top_out,top_amount')\n",
    "seq_h_out.shape, top_out.shape,top_amount.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQrhPqhafi_w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial freq stats: min=0.0e+00, max=4.2e+03, mean=3.2e+02, std=7.7e+02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000 — Avg Train Loss: 3.7496 | Avg Val Loss:   2.8391 | LR: 2.00e-05\n",
      "  ► Saved new best checkpoint (Val Loss: 2.8391)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10000 — Avg Train Loss: 2.8909 | Avg Val Loss:   2.7542 | LR: 4.00e-05\n",
      "  ► Saved new best checkpoint (Val Loss: 2.7542)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10000 — Avg Train Loss: 2.8145 | Avg Val Loss:   2.7496 | LR: 6.00e-05\n",
      "  ► Saved new best checkpoint (Val Loss: 2.7496)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10000 — Avg Train Loss: 2.8113 | Avg Val Loss:   2.7502 | LR: 8.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10000 — Avg Train Loss: 2.8025 | Avg Val Loss:   2.7496 | LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10000 — Avg Train Loss: 2.7989 | Avg Val Loss:   2.7516 | LR: 1.20e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10000 — Avg Train Loss: 2.8120 | Avg Val Loss:   2.7495 | LR: 1.40e-04\n",
      "  ► Saved new best checkpoint (Val Loss: 2.7495)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10000 — Avg Train Loss: 2.7989 | Avg Val Loss:   2.7495 | LR: 1.60e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10000 — Avg Train Loss: 2.7981 | Avg Val Loss:   2.7495 | LR: 1.80e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 2.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/10000 — Avg Train Loss: 2.7972 | Avg Val Loss:   2.7504 | LR: 2.20e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/10000 — Avg Train Loss: 2.7971 | Avg Val Loss:   2.7501 | LR: 2.40e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/10000 — Avg Train Loss: 2.7976 | Avg Val Loss:   2.7495 | LR: 2.60e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 2.80e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/10000 — Avg Train Loss: 2.7976 | Avg Val Loss:   2.7495 | LR: 3.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 3.20e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 3.40e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 3.60e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 3.80e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 4.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 4.20e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 4.40e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 4.60e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 4.80e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/10000 — Avg Train Loss: 2.7993 | Avg Val Loss:   2.7495 | LR: 5.00e-04\n",
      "  ► Saved new best checkpoint (Val Loss: 2.7495)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/10000 — Avg Train Loss: 2.7980 | Avg Val Loss:   2.7495 | LR: 5.20e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 5.40e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 5.60e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 5.80e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 6.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 6.20e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 6.40e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 6.60e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 6.80e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 7.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 7.20e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 7.40e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 7.60e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 7.80e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 8.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 8.20e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 8.40e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 8.60e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 8.80e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 9.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 9.20e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 9.40e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 9.60e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 9.80e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/10000 — Avg Train Loss: 2.7975 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 152/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 156/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 164/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 186/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 187/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 9.99e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 9.99e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 9.99e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 195/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 9.99e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 9.99e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 9.99e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 9.99e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 9.99e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 9.99e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 201/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 9.99e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 202/10000 — Avg Train Loss: 2.7973 | Avg Val Loss:   2.7495 | LR: 9.99e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 203/10000 [Train]:  52%|█████▏    | 26/50 [00:01<00:01, 17.82it/s, train_loss=2.7973]"
     ]
    }
   ],
   "source": [
    "# CLIPModel Pipeline with SimpleViT, BatchNorm, LayerNorm, Dropout, AMP, Scheduler, Early Stopping\n",
    "# This script processes genomic inputs (sequence + pattern + frequency) and trains a CLIP-like model using SimpleViT encoders\n",
    "\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# Import SimpleViT from lucidrains implementation\n",
    "from vit_pytorch.simple_vit import SimpleViT\n",
    "\n",
    "# Optional mixed precision\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "# -------------------- Flags & Hyperparameters --------------------\n",
    "LOAD_DATASET    = True      # If True, load from saved .npy files instead of generating\n",
    "SAVE_DATASET    = False     # If True (and generating), save arrays to .npy after generation\n",
    "\n",
    "n_epochs        = 10000\n",
    "batch_size      = 32\n",
    "learning_rate   = 1e-3               # adjust as needed\n",
    "weight_decay    = 5e-8               # weight decay for regularization\n",
    "dropout_prob    = 0.3                # dropout probability\n",
    "warmup_epochs   = 50                 # number of warmup epochs\n",
    "USE_AMP         = False              # set to True to enable mixed precision\n",
    "patience        = 1000               # early stopping patience\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. Load promoter coordinates from BED file\n",
    "# ==============================================================================\n",
    "promoter_bed = 'promoter_view_test.bed'  # replace with actual path\n",
    "bed_df = pd.read_csv(\n",
    "    promoter_bed,\n",
    "    sep='\\t',\n",
    "    header=None,\n",
    "    usecols=[0, 1, 2],\n",
    "    names=['chrom', 'start', 'end']\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Load intervals ±25 kb of promoter start (preprocessed into numpy arrays)\n",
    "# ==============================================================================\n",
    "# seq_h_dataset shape: (n_samples, 5, 32) → 5 rows, 32 nucleotides per row (one-hot or embedding channels)\n",
    "# top_out_dataset shape: (n_samples, 10, 100) → 10 pattern rows, 100 bp per pattern (binary patterns)\n",
    "# top_amount_dataset shape: (n_samples, 10) → frequency vector per pattern\n",
    "seq_h_dataset      = np.load('seq_h_dataset.npy')       # shape (n_samples, 5, 32)\n",
    "top_out_dataset    = np.load('top_out_dataset.npy')     # shape (n_samples, 10, 100)\n",
    "top_amount_dataset = np.load('top_amount_dataset.npy')  # shape (n_samples, 10)\n",
    "n_samples          = seq_h_dataset.shape[0]\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Split into Training (80%) and Validation (20%) Sets\n",
    "# ==============================================================================\n",
    "indices = np.arange(n_samples)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "split     = int(0.8 * n_samples)\n",
    "train_idx = indices[:split]\n",
    "val_idx   = indices[split:]\n",
    "\n",
    "train_seq_h      = seq_h_dataset[train_idx]\n",
    "train_top_out    = top_out_dataset[train_idx]\n",
    "train_top_amount = top_amount_dataset[train_idx]\n",
    "val_seq_h        = seq_h_dataset[val_idx]\n",
    "val_top_out      = top_out_dataset[val_idx]\n",
    "val_top_amount   = top_amount_dataset[val_idx]\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. Wrap in PyTorch Dataset and DataLoader\n",
    "# ==============================================================================\n",
    "class GenomicClipDataset(Dataset):\n",
    "    def __init__(self, seq_h_array, top_array, top_amount_array):\n",
    "        assert seq_h_array.shape[0] == top_array.shape[0] == top_amount_array.shape[0]\n",
    "        self.seq_h      = seq_h_array\n",
    "        self.top        = top_array\n",
    "        self.top_amount = top_amount_array\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.seq_h.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # dna: (5, 32) → convert channels dimension dynamically\n",
    "        dna  = torch.from_numpy(self.seq_h[idx]).float()       # (5, 32)\n",
    "        # patterns: (10, 100)\n",
    "        pat  = torch.from_numpy(self.top[idx]).float()         # (10, 100)\n",
    "        # frequency: (10,)\n",
    "        freq = torch.from_numpy(self.top_amount[idx]).float()  # (10,)\n",
    "        # Add channel dimension for ViT: dna → (channels, H, W)\n",
    "        # Here, treat '5' as channels or height? We treat 5 rows as height and 1 channel, so reshape to (1,5,32)\n",
    "        dna = dna.unsqueeze(0)  # → (1, 5, 32)\n",
    "        # For patterns: treat 10 rows as height, 1 channel, 100 width: reshape (1,10,100)\n",
    "        pat = pat.unsqueeze(0)  # → (1, 10, 100)\n",
    "        return dna, pat, freq\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    GenomicClipDataset(train_seq_h, train_top_out, train_top_amount),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    GenomicClipDataset(val_seq_h, val_top_out, val_top_amount),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. Define CLIPModel with SimpleViT encoders\n",
    "# ==============================================================================\n",
    "class CLIPModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dna_dim: int = 256,\n",
    "                 dna_depth: int = 10,\n",
    "                 dna_heads: int = 5,\n",
    "                 pat_dim: int = 2512,\n",
    "                 pat_depth: int = 10,\n",
    "                 pat_heads: int = 5,\n",
    "                 latent_dim: int = 2512,\n",
    "                 dropout: float = 0.4,\n",
    "                 dna_channels: int = 6,\n",
    "                 pat_channels: int = 6):\n",
    "        super(CLIPModel, self).__init__()\n",
    "\n",
    "        # --- DNA Encoder (SimpleViT) ---\n",
    "        # dna_tensor shape → (B, dna_channels, 5, 32)\n",
    "        self.dna_encoder = SimpleViT(\n",
    "            image_size=(5, 32),        # height x width for dna\n",
    "            patch_size=(5, 8),         # patch dims\n",
    "            num_classes=dna_dim,\n",
    "            dim=dna_dim,\n",
    "            depth=dna_depth,\n",
    "            heads=dna_heads,\n",
    "            mlp_dim=dna_dim * 2,\n",
    "            channels=dna_channels,\n",
    "            dim_head=64\n",
    "        )\n",
    "        self.dna_proj = nn.Sequential(\n",
    "            nn.Linear(dna_dim, latent_dim),\n",
    "            nn.LayerNorm(latent_dim),     # retain LayerNorm here\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),\n",
    "        )\n",
    "\n",
    "        # --- Pattern Encoder (SimpleViT) ---\n",
    "        # pat_tensor shape → (B, pat_channels, 10, 100)\n",
    "        self.pat_encoder = SimpleViT(\n",
    "            image_size=(10, 100),\n",
    "            patch_size=(2, 10),\n",
    "            num_classes=pat_dim,\n",
    "            dim=pat_dim,\n",
    "            depth=pat_depth,\n",
    "            heads=pat_heads,\n",
    "            mlp_dim=pat_dim * 2,\n",
    "            channels=pat_channels,\n",
    "            dim_head=64\n",
    "        )\n",
    "        self.pat_feat_bn = nn.BatchNorm1d(pat_dim, eps=1e-5, momentum=0.1)\n",
    "\n",
    "        # --- Frequency branch normalization ---\n",
    "        self.freq_bn = nn.BatchNorm1d(10, eps=1e-5, momentum=0.1)\n",
    "        self.freq_proj_input = nn.Linear(10, pat_dim)\n",
    "        self.freq_mlp = nn.Sequential(\n",
    "            nn.Linear(pat_dim, pat_dim),\n",
    "            nn.LayerNorm(pat_dim),       # keep LayerNorm inside freq MLP\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(pat_dim, pat_dim),\n",
    "            nn.LayerNorm(pat_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),\n",
    "        )\n",
    "\n",
    "        # --- Combiner MLP: (pat_dim*2 → pat_dim) with BatchNorm after first linear\n",
    "        self.combiner_mlp = nn.Sequential(\n",
    "            nn.Linear(pat_dim * 2, pat_dim),\n",
    "            nn.BatchNorm1d(pat_dim, eps=1e-5, momentum=0.1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(pat_dim, pat_dim),\n",
    "            nn.LayerNorm(pat_dim),       # final LayerNorm\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),\n",
    "        )\n",
    "\n",
    "        # Final projection: (pat_dim → latent_dim)\n",
    "        self.pat_proj = nn.Sequential(\n",
    "            nn.Linear(pat_dim, latent_dim),\n",
    "            nn.LayerNorm(latent_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),\n",
    "        )\n",
    "\n",
    "        # Learnable temperature parameter (initialized to log(1/0.07))\n",
    "        self.logit_scale_param = nn.Parameter(torch.ones([]) * math.log(1/0.07))\n",
    "\n",
    "        # Xavier initialization for all Linear layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, dna_tensor, pat_tensor, freq_tensor):\n",
    "        \"\"\"\n",
    "        dna_tensor:  (B, channels, 5, 32)\n",
    "        pat_tensor:  (B, channels, 10, 100)\n",
    "        freq_tensor: (B, 10)\n",
    "\n",
    "        Returns:\n",
    "            dna_latent: (B, latent_dim)\n",
    "            pat_latent: (B, latent_dim)\n",
    "            logit_scale: scalar > 0\n",
    "        \"\"\"\n",
    "        # --- DNA branch ---\n",
    "        dna_feat   = self.dna_encoder(dna_tensor)      # (B, dna_dim)\n",
    "        dna_latent = self.dna_proj(dna_feat)           # (B, latent_dim)\n",
    "\n",
    "        # --- Pattern branch ---\n",
    "        pat_feat       = self.pat_encoder(pat_tensor)  # (B, pat_dim)\n",
    "        pat_feat_norm  = self.pat_feat_bn(pat_feat)    # (B, pat_dim)\n",
    "\n",
    "        # --- Frequency branch ---\n",
    "        freq_normed = self.freq_bn(freq_tensor)       # (B, 10)\n",
    "        freq_proj   = self.freq_proj_input(freq_normed)  # (B, pat_dim)\n",
    "        freq_feat   = self.freq_mlp(freq_proj)           # (B, pat_dim)\n",
    "\n",
    "        # --- Combine pattern + frequency ---\n",
    "        combined   = torch.cat([pat_feat_norm, freq_feat], dim=1)  # (B, 2*pat_dim)\n",
    "        comb_feat  = self.combiner_mlp(combined)                   # (B, pat_dim)\n",
    "        pat_latent = self.pat_proj(comb_feat)                       # (B, latent_dim)\n",
    "\n",
    "        # Clamp logit_scale to avoid numerical explosion\n",
    "        logit_scale = torch.clamp(self.logit_scale_param, -5.0, 5.0).exp()\n",
    "        return dna_latent, pat_latent, logit_scale\n",
    "\n",
    "# Instantiate model and move to device (example dynamic channels)\n",
    "# dna_channels = number of input channels for DNA (e.g., 1 if one-hot encoded)\n",
    "# pat_channels = number of input channels for patterns (e.g., 1 for binary patterns)\n",
    "model = CLIPModel(\n",
    "    dna_dim=256, dna_depth=8, dna_heads=8,\n",
    "    pat_dim=512, pat_depth=8, pat_heads=8,\n",
    "    latent_dim=32, dropout=dropout_prob,\n",
    "    dna_channels=1, pat_channels=1\n",
    ").to(device)\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. Contrastive Loss (InfoNCE with Label Smoothing)\n",
    "# ==============================================================================\n",
    "def contrastive_loss(dna_latent, pat_latent, logit_scale, smoothing: float = 0.1):\n",
    "    \"\"\"\n",
    "    dna_latent:   (B, D)\n",
    "    pat_latent:   (B, D)\n",
    "    logit_scale:  scalar\n",
    "    smoothing:    float in [0,1]\n",
    "    Returns:\n",
    "        average of image-to-text and text-to-image KLDiv-based losses.\n",
    "    \"\"\"\n",
    "    B, D = dna_latent.size()\n",
    "    eps = 1e-8\n",
    "    dna_norm = dna_latent / (dna_latent.norm(dim=1, keepdim=True) + eps)\n",
    "    pat_norm = pat_latent / (pat_latent.norm(dim=1, keepdim=True) + eps)\n",
    "    logits = logit_scale * torch.matmul(dna_norm, pat_norm.t())  # (B, B)\n",
    "    labels = torch.arange(B, device=logits.device)\n",
    "    n_classes = logits.size(1)\n",
    "    with torch.no_grad():\n",
    "        smooth_target = torch.full((B, n_classes), smoothing / (n_classes - 1), device=logits.device)\n",
    "        smooth_target[torch.arange(B), labels] = 1.0 - smoothing\n",
    "    loss_i2p = F.kl_div(F.log_softmax(logits, dim=1), smooth_target, reduction=\"batchmean\")\n",
    "    loss_p2i = F.kl_div(F.log_softmax(logits.t(), dim=1), smooth_target, reduction=\"batchmean\")\n",
    "    return (loss_i2p + loss_p2i) / 2.0\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. Training & Validation Loop with Optional AMP, Extended Warmup, Early Stopping\n",
    "# ==============================================================================\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "total_steps  = n_epochs * len(train_loader)\n",
    "warmup_steps = warmup_epochs * len(train_loader)\n",
    "\n",
    "def lr_lambda(step):\n",
    "    if step < warmup_steps:\n",
    "        return float(step) / float(max(1, warmup_steps))\n",
    "    return 0.5 * (1.0 + math.cos(math.pi * (step - warmup_steps) / (total_steps - warmup_steps)))\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "scaler = GradScaler(enabled=USE_AMP)\n",
    "\n",
    "# Debug: frequency stats\n",
    "freq_all = torch.from_numpy(top_amount_dataset)\n",
    "print(\n",
    "    \"Initial freq stats:\",\n",
    "    f\"min={freq_all.min().item():.1e}, max={freq_all.max().item():.1e},\",\n",
    "    f\"mean={freq_all.mean().item():.1e}, std={freq_all.std().item():.1e}\"\n",
    ")\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "no_improve    = 0\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # --- Training Phase ---\n",
    "    model.train()\n",
    "    train_loss_accum = 0.0\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{n_epochs} [Train]\", leave=False)\n",
    "\n",
    "    for dna_batch, pat_batch, freq_batch in train_bar:\n",
    "        dna_batch, pat_batch, freq_batch = dna_batch.to(device), pat_batch.to(device), freq_batch.to(device)\n",
    "        if dna_batch.isnan().any() or dna_batch.isinf().any():\n",
    "            raise RuntimeError(\"dna_batch contains NaN or Inf\")\n",
    "        if pat_batch.isnan().any() or pat_batch.isinf().any():\n",
    "            raise RuntimeError(\"pat_batch contains NaN or Inf\")\n",
    "        if freq_batch.isnan().any() or freq_batch.isinf().any():\n",
    "            raise RuntimeError(\"freq_batch contains NaN or Inf\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if USE_AMP:\n",
    "            with autocast():\n",
    "                dna_latent, pat_latent, logit_scale = model(dna_batch, pat_batch, freq_batch)\n",
    "                loss = contrastive_loss(dna_latent, pat_latent, logit_scale)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "        else:\n",
    "            dna_latent, pat_latent, logit_scale = model(dna_batch, pat_batch, freq_batch)\n",
    "            loss = contrastive_loss(dna_latent, pat_latent, logit_scale)\n",
    "            loss.backward()\n",
    "\n",
    "        # Gradient clipping & check\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None and param.grad.isnan().any():\n",
    "                raise RuntimeError(f\"Gradient for {name} contains NaN\")\n",
    "\n",
    "        if USE_AMP:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss_accum += loss.item()\n",
    "        train_bar.set_postfix(train_loss=f\"{loss.item():.4f}\")\n",
    "    train_bar.close()\n",
    "\n",
    "    avg_train_loss = train_loss_accum / len(train_loader)\n",
    "    model.eval()\n",
    "    val_loss_accum = 0.0\n",
    "    val_bar = tqdm(val_loader, desc=f\"Epoch {epoch}/{n_epochs} [Val]\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for dna_batch, pat_batch, freq_batch in val_bar:\n",
    "            dna_batch, pat_batch, freq_batch = dna_batch.to(device), pat_batch.to(device), freq_batch.to(device)\n",
    "            if USE_AMP:\n",
    "                with autocast():\n",
    "                    dna_latent, pat_latent, logit_scale = model(dna_batch, pat_batch, freq_batch)\n",
    "                    loss = contrastive_loss(dna_latent, pat_latent, logit_scale)\n",
    "            else:\n",
    "                dna_latent, pat_latent, logit_scale = model(dna_batch, pat_batch, freq_batch)\n",
    "                loss = contrastive_loss(dna_latent, pat_latent, logit_scale)\n",
    "            val_loss_accum += loss.item()\n",
    "            val_bar.set_postfix(val_loss=f\"{loss.item():.4f}\")\n",
    "    val_bar.close()\n",
    "\n",
    "    avg_val_loss = val_loss_accum / len(val_loader)\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{n_epochs} — \"\n",
    "        f\"Avg Train Loss: {avg_train_loss:.4f} | \"\n",
    "        f\"Avg Val Loss:   {avg_val_loss:.4f} | \"\n",
    "        f\"LR: {scheduler.get_last_lr()[0]:.2e}\"\n",
    "    )\n",
    "\n",
    "    # Early Stopping & Checkpoint\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        no_improve    = 0\n",
    "        ckpt_path     = os.path.join(checkpoint_dir, \"best_clip_model.pth\")\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"scheduler_state\": scheduler.state_dict(),\n",
    "            \"best_val_loss\": best_val_loss\n",
    "        }, ckpt_path)\n",
    "        print(f\"  ► Saved new best checkpoint (Val Loss: {best_val_loss:.4f})\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f\"No improvement for {patience} epochs. Stopping early at epoch {epoch}.\")\n",
    "            break\n",
    "\n",
    "# ==============================================================================\n",
    "# 8. Save Final Model Checkpoint (if not already saved in early stop)\n",
    "# ==============================================================================\n",
    "final_ckpt = os.path.join(checkpoint_dir, \"last_epoch_clip_model.pth\")\n",
    "torch.save({\n",
    "    \"epoch\": epoch,\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"optimizer_state\": optimizer.state_dict(),\n",
    "    \"scheduler_state\": scheduler.state_dict(),\n",
    "    \"best_val_loss\": best_val_loss\n",
    "}, final_ckpt)\n",
    "print(f\"Training complete. Final checkpoint saved to {final_ckpt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZSXsTSQsryS"
   },
   "outputs": [],
   "source": [
    "# Overfit is working\n",
    "# dropout is regulating it right now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xaEJJkErP7xv"
   },
   "outputs": [],
   "source": [
    "#clip\n",
    "first modality\n",
    "\n",
    "DNA 2d matrix\n",
    "  - 32 bp binary hotencoder ((5, 32)) #ACTGN\n",
    "\n",
    "Second clip MODALITY\n",
    "\n",
    "2d matrix (peak binarized top 10 encoders) + 1d vector with values (frequency of the top 10 encoders)\n",
    "\n",
    "  - (10,100) binary sparse_matrix\n",
    "  - frequency of occurency for values for each row # This information should be used to reflect the frequency\n",
    "\n",
    "\n",
    "use VIT to process both modules,\n",
    "add batch norm when needed\n",
    "use the lucid rains implementation of x-clip and VIT\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfnZyMG0Y08A"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
